# ğŸ¦™ Provider LOCAL - Guide d'utilisation

## Vue d'ensemble

Le **provider LOCAL** permet d'utiliser des modÃ¨les de langage (LLM) qui tournent localement sur votre machine. Cette fonctionnalitÃ© offre plusieurs avantages :

- ğŸ”’ **ConfidentialitÃ© totale** : Vos donnÃ©es ne quittent jamais votre machine
- ğŸ’° **Aucun coÃ»t d'API** : Pas de frais d'utilisation comme avec les services cloud
- ğŸ›ï¸ **ContrÃ´le total** : Choix du modÃ¨le, paramÃ¨tres personnalisÃ©s
- ğŸš€ **Latence rÃ©duite** : Pas de dÃ©lai de rÃ©seau
- ğŸŒ **DisponibilitÃ© offline** : Fonctionne sans connexion internet

## Solutions supportÃ©es

Le provider LOCAL est compatible avec toute solution exposant une **API compatible OpenAI** :

### ğŸ¦™ Ollama (RecommandÃ©)
- **URL par dÃ©faut** : `http://localhost:11434`
- **Installation** : [ollama.ai](https://ollama.ai/)
- **Avantages** : Simple, lÃ©ger, gestion automatique des modÃ¨les
- **Commandes de base** :
  ```bash
  # Installation d'un modÃ¨le
  ollama pull llama3.2
  
  # DÃ©marrage interactif
  ollama run llama3.2
  
  # DÃ©marrage en arriÃ¨re-plan
  ollama serve
  ```

### ğŸ–¥ï¸ LM Studio
- **URL par dÃ©faut** : `http://localhost:1234`
- **Installation** : [lmstudio.ai](https://lmstudio.ai/)
- **Avantages** : Interface graphique, gestion facile des modÃ¨les

### ğŸ Text Generation WebUI (oobabooga)
- **URL par dÃ©faut** : `http://localhost:5000` (ou custom)
- **Installation** : [GitHub](https://github.com/oobabooga/text-generation-webui)
- **Avantages** : Interface web complÃ¨te, nombreux paramÃ¨tres

## Configuration

### 1. Installation du serveur local

**Option A - Ollama (RecommandÃ© pour dÃ©butants)** :
```bash
# TÃ©lÃ©charger depuis https://ollama.ai/
# Puis installer un modÃ¨le
ollama pull llama3.2
ollama run llama3.2
```

**Option B - LM Studio** :
1. TÃ©lÃ©charger depuis [lmstudio.ai](https://lmstudio.ai/)
2. TÃ©lÃ©charger un modÃ¨le depuis l'interface
3. DÃ©marrer le serveur local

### 2. Configuration dans BGRAPP Pyconseil

1. **Lancer l'interface de configuration** :
   ```bash
   python demo_config_local.py
   ```

2. **Dans l'interface** :
   - SÃ©lectionner l'onglet **"LLM Local"**
   - Configurer l'**URL du serveur** (ex: `http://localhost:11434`)
   - Choisir le **modÃ¨le** installÃ©
   - Cocher **"Utiliser LLM Local"**
   - Cliquer sur **"Tester la connexion"**
   - **Sauvegarder** la configuration

### 3. ModÃ¨les recommandÃ©s

| ModÃ¨le | Taille | RAM requise | Vitesse | QualitÃ© |
|--------|---------|-------------|---------|---------|
| `llama3.2` | 3B | 4 GB | âš¡âš¡âš¡ | â­â­â­ |
| `llama3.1` | 8B | 8 GB | âš¡âš¡ | â­â­â­â­ |
| `mistral` | 7B | 8 GB | âš¡âš¡ | â­â­â­â­ |
| `qwen2.5` | 7B | 8 GB | âš¡âš¡ | â­â­â­â­ |
| `codellama` | 7B | 8 GB | âš¡âš¡ | â­â­â­â­â­ |

## Test et validation

### Test automatique
```bash
# Test complet de l'intÃ©gration
python demo_test_local_provider.py

# DÃ©monstration avec interface
python demo_config_local.py
```

### Test manuel
1. **VÃ©rifier que le serveur local tourne** :
   ```bash
   curl http://localhost:11434/api/version  # Pour Ollama
   curl http://localhost:1234/v1/models    # Pour LM Studio
   ```

2. **Tester dans l'interface** :
   - Ouvrir l'interface de configuration
   - Aller sur l'onglet "LLM Local"
   - Cliquer sur "Tester la connexion"

## Utilisation

Une fois configurÃ©, le provider LOCAL fonctionne exactement comme les autres providers :

```python
from src.services.openai_service import get_ai_service
from src.services.ai_config_service import AIProvider

# Obtenir le service IA configurÃ© pour LOCAL
ai_service = get_ai_service(provider=AIProvider.LOCAL)

# Utiliser comme un service IA standard
appreciation = ai_service.preprocess_appreciation(
    "L'Ã©lÃ¨ve fait de bons efforts en mathÃ©matiques.",
    student_nom="Dupont",
    student_prenom="Marie"
)
```

## DÃ©pannage

### âŒ "Impossible de se connecter au serveur local"
- **VÃ©rifiez que le serveur est dÃ©marrÃ©** : `ollama list` ou vÃ©rifier LM Studio
- **VÃ©rifiez l'URL** : Par dÃ©faut `http://localhost:11434` pour Ollama
- **VÃ©rifiez le port** : Peut Ãªtre occupÃ© par un autre processus

### âŒ "ModÃ¨le non trouvÃ©"
- **Ollama** : `ollama pull nom_du_modele`
- **LM Studio** : TÃ©lÃ©charger le modÃ¨le via l'interface
- **VÃ©rifiez le nom exact** : `ollama list` pour voir les modÃ¨les disponibles

### âŒ "Client OpenAI non installÃ©"
```bash
pip install openai>=1.0.0
```

### âŒ Performances lentes
- **VÃ©rifiez la RAM disponible** : Fermez les applications inutiles
- **Utilisez un modÃ¨le plus petit** : `llama3.2` au lieu de `llama3.1`
- **GPU** : Certaines solutions supportent l'accÃ©lÃ©ration GPU

## Avantages vs inconvÃ©nients

### âœ… Avantages
- **ConfidentialitÃ© maximale** : DonnÃ©es privÃ©es
- **Pas de coÃ»ts d'API** : Utilisation illimitÃ©e
- **Latence faible** : Pas de rÃ©seau
- **ContrÃ´le total** : ParamÃ¨tres personnalisables
- **DisponibilitÃ© offline** : Fonctionne sans internet

### âš ï¸ InconvÃ©nients
- **Ressources locales** : NÃ©cessite RAM/CPU suffisants
- **Configuration initiale** : Plus complexe que les APIs cloud
- **QualitÃ© variable** : DÃ©pend du modÃ¨le choisi
- **Maintenance** : Mises Ã  jour manuelles des modÃ¨les

## SÃ©curitÃ© et confidentialitÃ©

Le provider LOCAL garantit une **confidentialitÃ© totale** :

- âœ… **Aucune donnÃ©e transmise** vers des serveurs externes
- âœ… **Traitement 100% local** sur votre machine
- âœ… **ConformitÃ© RGPD native** : Les donnÃ©es ne quittent pas votre environnement
- âœ… **Pas de logs externes** : Aucun historique chez des tiers
- âœ… **ContrÃ´le total** sur le stockage et le traitement

## Support et communautÃ©

- ğŸ“– **Documentation Ollama** : [docs.ollama.ai](https://docs.ollama.ai/)
- ğŸ’¬ **CommunautÃ© Ollama** : [GitHub Discussions](https://github.com/ollama/ollama/discussions)
- ğŸ› **Signaler un bug** : CrÃ©er une issue dans le projet BGRAPP
- ğŸ’¡ **Suggestions** : Proposer des amÃ©liorations

---

*Le provider LOCAL transforme votre machine en une puissante plateforme IA privÃ©e et gratuite ! ğŸš€* 