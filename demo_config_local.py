#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DÃ©monstration de la configuration du provider LOCAL
Test du LLM local via l'interface de configuration
"""

import sys
import os
from pathlib import Path

# Ajouter le dossier src au path pour les imports
sys.path.insert(0, str(Path(__file__).parent / 'src'))

def demo_config_local():
    """DÃ©monstration de la configuration LOCAL"""
    
    print("=" * 60)
    print("ğŸ¦™ DÃ‰MONSTRATION - Configuration LOCAL")
    print("=" * 60)
    
    try:
        from src.services.ai_config_service import AIProvider, get_ai_config_service
        from src.gui.config_window import ConfigWindow
        
        print("\nğŸ“Š Ã‰tat initial de la configuration LOCAL:")
        config_service = get_ai_config_service()
        
        # Afficher les informations LOCAL
        models = config_service.get_available_models(AIProvider.LOCAL)
        print(f"  ğŸ¤– ModÃ¨les LOCAL disponibles: {models}")
        
        default_model = config_service.get_model(AIProvider.LOCAL)
        print(f"  ğŸ¯ ModÃ¨le par dÃ©faut: {default_model}")
        
        base_url = config_service.get_base_url(AIProvider.LOCAL)
        print(f"  ğŸŒ URL par dÃ©faut: {base_url}")
        
        # VÃ©rifier la validation
        validation = config_service.validate_configuration()
        local_available = AIProvider.LOCAL in validation['providers_with_keys']
        print(f"  âœ… LOCAL disponible: {local_available}")
        
        print("\nğŸ’¡ Instructions pour utiliser LOCAL:")
        print("  1. ğŸ”§ Installez Ollama depuis https://ollama.ai/")
        print("  2. ğŸ“¥ TÃ©lÃ©chargez un modÃ¨le: 'ollama pull llama3.2'")
        print("  3. â–¶ï¸ DÃ©marrez le modÃ¨le: 'ollama run llama3.2'")
        print("  4. ğŸŒ VÃ©rifiez que le serveur tourne sur http://localhost:11434")
        print("  5. ğŸ–¥ï¸ Utilisez l'interface ci-dessous pour configurer et tester")
        
        print("\nğŸ–¥ï¸ Lancement de l'interface de configuration...")
        print("   ğŸ‘‰ SÃ©lectionnez l'onglet 'LLM Local'")
        print("   ğŸ‘‰ VÃ©rifiez l'URL du serveur")
        print("   ğŸ‘‰ SÃ©lectionnez le modÃ¨le")
        print("   ğŸ‘‰ Activez le provider LOCAL")
        print("   ğŸ‘‰ Testez la connexion")
        
        def on_config_changed():
            """Callback appelÃ© quand la configuration change"""
            print("\nğŸ”§ Configuration LOCAL mise Ã  jour!")
            new_summary = config_service.get_current_config_summary()
            if new_summary['enabled_provider'] == AIProvider.LOCAL:
                print("  ğŸ¯ LOCAL est maintenant le fournisseur actif!")
        
        # CrÃ©er et lancer la fenÃªtre de configuration
        config_window = ConfigWindow(on_config_changed=on_config_changed)
        
        # Mettre l'onglet LOCAL en avant
        print("\nğŸ“‹ Conseils pour la configuration:")
        print("  â€¢ URL Ollama standard: http://localhost:11434")
        print("  â€¢ URL LM Studio standard: http://localhost:1234")
        print("  â€¢ Pas de clÃ© API nÃ©cessaire pour LOCAL")
        print("  â€¢ SÃ©lectionnez le modÃ¨le que vous avez installÃ©")
        
        config_window.run()
        
        print("\nâœ… DÃ©monstration terminÃ©e!")
        
        # Afficher l'Ã©tat final
        final_summary = config_service.get_current_config_summary()
        if final_summary['enabled_provider'] == AIProvider.LOCAL:
            print("ğŸ‰ LOCAL configurÃ© avec succÃ¨s comme fournisseur actif!")
            base_url = final_summary['base_urls'].get(AIProvider.LOCAL, 'Non configurÃ©')
            model = final_summary['models'].get(AIProvider.LOCAL, 'Non configurÃ©')
            print(f"  ğŸŒ URL: {base_url}")
            print(f"  ğŸ¤– ModÃ¨le: {model}")
        else:
            print("â„¹ï¸ LOCAL n'est pas le fournisseur actif")
        
    except ImportError as e:
        print(f"âŒ Erreur d'import: {e}")
        print("VÃ©rifiez que tous les modules sont installÃ©s")
    except Exception as e:
        print(f"âŒ Erreur lors de la dÃ©monstration: {e}")
        import traceback
        traceback.print_exc()


def test_local_quick():
    """Test rapide de la configuration LOCAL"""
    
    print("\nğŸ§ª Test rapide LOCAL...")
    
    try:
        from src.services.ai_config_service import AIProvider, get_ai_config_service
        from src.services.ai_connection_test_service import get_ai_connection_test_service
        
        config_service = get_ai_config_service()
        test_service = get_ai_connection_test_service()
        
        # Configurer LOCAL
        config_service.set_base_url(AIProvider.LOCAL, "http://localhost:11434")
        config_service.set_model(AIProvider.LOCAL, "llama3.2")
        config_service.set_enabled_provider(AIProvider.LOCAL)
        
        print("  âœ… Configuration LOCAL sauvegardÃ©e")
        
        # Test de connexion
        print("  ğŸ”„ Test de connexion au serveur local...")
        result = test_service.test_connection(
            AIProvider.LOCAL,
            "http://localhost:11434",
            "llama3.2"
        )
        
        if result.success:
            print("  ğŸ‰ Connexion rÃ©ussie au serveur local!")
            print(f"     DÃ©tails: {result.message}")
        else:
            print("  âš ï¸ Connexion Ã©chouÃ©e (normal si serveur non dÃ©marrÃ©)")
            print(f"     Erreur: {result.message}")
        
        return result.success
        
    except Exception as e:
        print(f"  âŒ Erreur test: {e}")
        return False


if __name__ == "__main__":
    print("ğŸš€ DÃ©monstration configuration LOCAL...")
    
    # Test rapide d'abord
    connection_ok = test_local_quick()
    
    if connection_ok:
        print("\nğŸ‰ Serveur local dÃ©tectÃ© et fonctionnel!")
        print("Vous pouvez utiliser LOCAL directement.")
    else:
        print("\nâš ï¸ Aucun serveur local dÃ©tectÃ©.")
        print("La dÃ©monstration va vous montrer comment configurer LOCAL.")
    
    # Lancement de l'interface
    demo_config_local() 